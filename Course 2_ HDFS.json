{"paragraphs":[{"title":"Haddop Worksop - Part 1 - HDSF","text":"# sudo service zeppelin restart\n\nhdfs --help","user":"anonymous","dateUpdated":"2021-09-30T09:37:29-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Usage: hdfs [--config confdir] COMMAND\n       where COMMAND is one of:\n  dfs                  run a filesystem command on the file systems supported in Hadoop.\n  namenode -format     format the DFS filesystem\n  secondarynamenode    run the DFS secondary namenode\n  namenode             run the DFS namenode\n  journalnode          run the DFS journalnode\n  zkfc                 run the ZK Failover Controller daemon\n  datanode             run a DFS datanode\n  dfsadmin             run a DFS admin client\n  diskbalancer         Distributes data evenly among disks on a given node\n  haadmin              run a DFS HA admin client\n  fsck                 run a DFS filesystem checking utility\n  balancer             run a cluster balancing utility\n  jmxget               get JMX exported values from NameNode or DataNode.\n  mover                run a utility to move block replicas across\n                       storage types\n  oiv                  apply the offline fsimage viewer to an fsimage\n  oiv_legacy           apply the offline fsimage viewer to an legacy fsimage\n  oev                  apply the offline edits viewer to an edits file\n  fetchdt              fetch a delegation token from the NameNode\n  getconf              get config values from configuration\n  groups               get the groups which users belong to\n  snapshotDiff         diff two snapshots of a directory or diff the\n                       current directory contents with a snapshot\n  lsSnapshottableDir   list all snapshottable dirs owned by the current user\n\t\t\t\t\t\tUse -help to see options\n  portmap              run a portmap service\n  nfs3                 run an NFS version 3 gateway\n  cacheadmin           configure the HDFS cache\n  crypto               configure HDFS encryption zones\n  storagepolicies      list/get/set block storage policies\n  version              print the version\n\nMost commands print help when invoked w/o parameters.\n"}]},"apps":[],"jobName":"paragraph_1632954431119_-2141078216","id":"20210929-152711_2005744768","dateCreated":"2021-09-29T15:27:11-0700","dateStarted":"2021-09-29T15:27:29-0700","dateFinished":"2021-09-29T15:27:29-0700","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:33527"},{"text":"hdfs dfs -help","user":"anonymous","dateUpdated":"2021-09-29T15:27:37-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Usage: hadoop fs [generic options]\n\t[-appendToFile <localsrc> ... <dst>]\n\t[-cat [-ignoreCrc] <src> ...]\n\t[-checksum <src> ...]\n\t[-chgrp [-R] GROUP PATH...]\n\t[-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH...]\n\t[-chown [-R] [OWNER][:[GROUP]] PATH...]\n\t[-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst>]\n\t[-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-count [-q] [-h] [-v] [-x] <path> ...]\n\t[-cp [-f] [-p | -p[topax]] <src> ... <dst>]\n\t[-createSnapshot <snapshotDir> [<snapshotName>]]\n\t[-deleteSnapshot <snapshotDir> <snapshotName>]\n\t[-df [-h] [<path> ...]]\n\t[-du [-s] [-h] [-x] <path> ...]\n\t[-expunge]\n\t[-find <path> ... <expression> ...]\n\t[-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst>]\n\t[-getfacl [-R] <path>]\n\t[-getfattr [-R] {-n name | -d} [-e en] <path>]\n\t[-getmerge [-nl] <src> <localdst>]\n\t[-help [cmd ...]]\n\t[-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...]]\n\t[-mkdir [-p] <path> ...]\n\t[-moveFromLocal <localsrc> ... <dst>]\n\t[-moveToLocal <src> <localdst>]\n\t[-mv <src> ... <dst>]\n\t[-put [-f] [-p] [-l] <localsrc> ... <dst>]\n\t[-renameSnapshot <snapshotDir> <oldName> <newName>]\n\t[-rm [-f] [-r|-R] [-skipTrash] <src> ...]\n\t[-rmdir [--ignore-fail-on-non-empty] <dir> ...]\n\t[-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>]]\n\t[-setfattr {-n name [-v value] | -x name} <path>]\n\t[-setrep [-R] [-w] <rep> <path> ...]\n\t[-stat [format] <path> ...]\n\t[-tail [-f] <file>]\n\t[-test -[defsz] <path>]\n\t[-text [-ignoreCrc] <src> ...]\n\t[-touchz <path> ...]\n\t[-usage [cmd ...]]\n\n-appendToFile <localsrc> ... <dst> :\n  Appends the contents of all the given local files to the given dst file. The dst\n  file will be created if it does not exist. If <localSrc> is -, then the input is\n  read from stdin.\n\n-cat [-ignoreCrc] <src> ... :\n  Fetch all files that match the file pattern <src> and display their content on\n  stdout.\n\n-checksum <src> ... :\n  Dump checksum information for files that match the file pattern <src> to stdout.\n  Note that this requires a round-trip to a datanode storing each block of the\n  file, and thus is not efficient to run on a large number of files. The checksum\n  of a file depends on its content, block size and the checksum algorithm and\n  parameters used for creating the file.\n\n-chgrp [-R] GROUP PATH... :\n  This is equivalent to -chown ... :GROUP ...\n\n-chmod [-R] <MODE[,MODE]... | OCTALMODE> PATH... :\n  Changes permissions of a file. This works similar to the shell's chmod command\n  with a few exceptions.\n                                                                                 \n  -R           modifies the files recursively. This is the only option currently \n               supported.                                                        \n  <MODE>       Mode is the same as mode used for the shell's command. The only   \n               letters recognized are 'rwxXt', e.g. +t,a+r,g-w,+rwx,o=r.         \n  <OCTALMODE>  Mode specifed in 3 or 4 digits. If 4 digits, the first may be 1 or\n               0 to turn the sticky bit on or off, respectively.  Unlike the     \n               shell command, it is not possible to specify only part of the     \n               mode, e.g. 754 is same as u=rwx,g=rx,o=r.                         \n  \n  If none of 'augo' is specified, 'a' is assumed and unlike the shell command, no\n  umask is applied.\n\n-chown [-R] [OWNER][:[GROUP]] PATH... :\n  Changes owner and group of a file. This is similar to the shell's chown command\n  with a few exceptions.\n                                                                                 \n  -R  modifies the files recursively. This is the only option currently          \n      supported.                                                                 \n  \n  If only the owner or group is specified, then only the owner or group is\n  modified. The owner and group names may only consist of digits, alphabet, and\n  any of [-_./@a-zA-Z0-9]. The names are case sensitive.\n  \n  WARNING: Avoid using '.' to separate user name and group though Linux allows it.\n  If user names have dots in them and you are using local file system, you might\n  see surprising results since the shell command 'chown' is used for local files.\n\n-copyFromLocal [-f] [-p] [-l] <localsrc> ... <dst> :\n  Identical to the -put command.\n\n-copyToLocal [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\n  Identical to the -get command.\n\n-count [-q] [-h] [-v] [-x] <path> ... :\n  Count the number of directories, files and bytes under the paths\n  that match the specified file pattern.  The output columns are:\n  DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n  or, with the -q option:\n  QUOTA REM_QUOTA SPACE_QUOTA REM_SPACE_QUOTA\n        DIR_COUNT FILE_COUNT CONTENT_SIZE PATHNAME\n  The -h option shows file sizes in human readable format.\n  The -v option displays a header line.\n  The -x option excludes snapshots from being calculated.\n\n-cp [-f] [-p | -p[topax]] <src> ... <dst> :\n  Copy files that match the file pattern <src> to a destination.  When copying\n  multiple files, the destination must be a directory. Passing -p preserves status\n  [topax] (timestamps, ownership, permission, ACLs, XAttr). If -p is specified\n  with no <arg>, then preserves timestamps, ownership, permission. If -pa is\n  specified, then preserves permission also because ACL is a super-set of\n  permission. Passing -f overwrites the destination if it already exists. raw\n  namespace extended attributes are preserved if (1) they are supported (HDFS\n  only) and, (2) all of the source and target pathnames are in the /.reserved/raw\n  hierarchy. raw namespace xattr preservation is determined solely by the presence\n  (or absence) of the /.reserved/raw prefix and not by the -p option.\n\n-createSnapshot <snapshotDir> [<snapshotName>] :\n  Create a snapshot on a directory\n\n-deleteSnapshot <snapshotDir> <snapshotName> :\n  Delete a snapshot from a directory\n\n-df [-h] [<path> ...] :\n  Shows the capacity, free and used space of the filesystem. If the filesystem has\n  multiple partitions, and no path to a particular partition is specified, then\n  the status of the root partitions will be shown.\n                                                                                 \n  -h  Formats the sizes of files in a human-readable fashion rather than a number\n      of bytes.                                                                  \n\n-du [-s] [-h] [-x] <path> ... :\n  Show the amount of space, in bytes, used by the files that match the specified\n  file pattern. The following flags are optional:\n                                                                                 \n  -s  Rather than showing the size of each individual file that matches the      \n      pattern, shows the total (summary) size.                                   \n  -h  Formats the sizes of files in a human-readable fashion rather than a number\n      of bytes.                                                                  \n  -x  Excludes snapshots from being counted.                                     \n  \n  Note that, even without the -s option, this only shows size summaries one level\n  deep into a directory.\n  \n  The output is in the form \n  \tsize\tdisk space consumed\tname(full path)\n\n-expunge :\n  Empty the Trash\n\n-find <path> ... <expression> ... :\n  Finds all files that match the specified expression and\n  applies selected actions to them. If no <path> is specified\n  then defaults to the current working directory. If no\n  expression is specified then defaults to -print.\n  \n  The following primary expressions are recognised:\n    -name pattern\n    -iname pattern\n      Evaluates as true if the basename of the file matches the\n      pattern using standard file system globbing.\n      If -iname is used then the match is case insensitive.\n  \n    -print\n    -print0\n      Always evaluates to true. Causes the current pathname to be\n      written to standard output followed by a newline. If the -print0\n      expression is used then an ASCII NULL character is appended rather\n      than a newline.\n  \n  The following operators are recognised:\n    expression -a expression\n    expression -and expression\n    expression expression\n      Logical AND operator for joining two expressions. Returns\n      true if both child expressions return true. Implied by the\n      juxtaposition of two expressions and so does not need to be\n      explicitly specified. The second expression will not be\n      applied if the first fails.\n\n-get [-p] [-ignoreCrc] [-crc] <src> ... <localdst> :\n  Copy files that match the file pattern <src> to the local name.  <src> is kept. \n  When copying multiple files, the destination must be a directory. Passing -p\n  preserves access and modification times, ownership and the mode.\n\n-getfacl [-R] <path> :\n  Displays the Access Control Lists (ACLs) of files and directories. If a\n  directory has a default ACL, then getfacl also displays the default ACL.\n                                                                  \n  -R      List the ACLs of all files and directories recursively. \n  <path>  File or directory to list.                              \n\n-getfattr [-R] {-n name | -d} [-e en] <path> :\n  Displays the extended attribute names and values (if any) for a file or\n  directory.\n                                                                                 \n  -R             Recursively list the attributes for all files and directories.  \n  -n name        Dump the named extended attribute value.                        \n  -d             Dump all extended attribute values associated with pathname.    \n  -e <encoding>  Encode values after retrieving them.Valid encodings are \"text\", \n                 \"hex\", and \"base64\". Values encoded as text strings are enclosed\n                 in double quotes (\"), and values encoded as hexadecimal and     \n                 base64 are prefixed with 0x and 0s, respectively.               \n  <path>         The file or directory.                                          \n\n-getmerge [-nl] <src> <localdst> :\n  Get all the files in the directories that match the source file pattern and\n  merge and sort them to only one file on local fs. <src> is kept.\n                                                        \n  -nl  Add a newline character at the end of each file. \n\n-help [cmd ...] :\n  Displays help for given command or all commands if none is specified.\n\n-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path> ...] :\n  List the contents that match the specified file pattern. If path is not\n  specified, the contents of /user/<currentUser> will be listed. For a directory a\n  list of its direct children is returned (unless -d option is specified).\n  \n  Directory entries are of the form:\n  \tpermissions - userId groupId sizeOfDirectory(in bytes)\n  modificationDate(yyyy-MM-dd HH:mm) directoryName\n  \n  and file entries are of the form:\n  \tpermissions numberOfReplicas userId groupId sizeOfFile(in bytes)\n  modificationDate(yyyy-MM-dd HH:mm) fileName\n  \n    -C  Display the paths of files and directories only.\n    -d  Directories are listed as plain files.\n    -h  Formats the sizes of files in a human-readable fashion\n        rather than a number of bytes.\n    -q  Print ? instead of non-printable characters.\n    -R  Recursively list the contents of directories.\n    -t  Sort files by modification time (most recent first).\n    -S  Sort files by size.\n    -r  Reverse the order of the sort.\n    -u  Use time of last access instead of modification for\n        display and sorting.\n\n-mkdir [-p] <path> ... :\n  Create a directory in specified location.\n                                                  \n  -p  Do not fail if the directory already exists \n\n-moveFromLocal <localsrc> ... <dst> :\n  Same as -put, except that the source is deleted after it's copied.\n\n-moveToLocal <src> <localdst> :\n  Not implemented yet\n\n-mv <src> ... <dst> :\n  Move files that match the specified file pattern <src> to a destination <dst>. \n  When moving multiple files, the destination must be a directory.\n\n-put [-f] [-p] [-l] <localsrc> ... <dst> :\n  Copy files from the local file system into fs. Copying fails if the file already\n  exists, unless the -f flag is given.\n  Flags:\n                                                                       \n  -p  Preserves access and modification times, ownership and the mode. \n  -f  Overwrites the destination if it already exists.                 \n  -l  Allow DataNode to lazily persist the file to disk. Forces        \n         replication factor of 1. This flag will result in reduced\n         durability. Use with care.\n\n-renameSnapshot <snapshotDir> <oldName> <newName> :\n  Rename a snapshot from oldName to newName\n\n-rm [-f] [-r|-R] [-skipTrash] <src> ... :\n  Delete all files that match the specified file pattern. Equivalent to the Unix\n  command \"rm <src>\"\n                                                                                 \n  -skipTrash  option bypasses trash, if enabled, and immediately deletes <src>   \n  -f          If the file does not exist, do not display a diagnostic message or \n              modify the exit status to reflect an error.                        \n  -[rR]       Recursively deletes directories                                    \n\n-rmdir [--ignore-fail-on-non-empty] <dir> ... :\n  Removes the directory entry specified by each directory argument, provided it is\n  empty.\n\n-setfacl [-R] [{-b|-k} {-m|-x <acl_spec>} <path>]|[--set <acl_spec> <path>] :\n  Sets Access Control Lists (ACLs) of files and directories.\n  Options:\n                                                                                 \n  -b          Remove all but the base ACL entries. The entries for user, group   \n              and others are retained for compatibility with permission bits.    \n  -k          Remove the default ACL.                                            \n  -R          Apply operations to all files and directories recursively.         \n  -m          Modify ACL. New entries are added to the ACL, and existing entries \n              are retained.                                                      \n  -x          Remove specified ACL entries. Other ACL entries are retained.      \n  --set       Fully replace the ACL, discarding all existing entries. The        \n              <acl_spec> must include entries for user, group, and others for    \n              compatibility with permission bits.                                \n  <acl_spec>  Comma separated list of ACL entries.                               \n  <path>      File or directory to modify.                                       \n\n-setfattr {-n name [-v value] | -x name} <path> :\n  Sets an extended attribute name and value for a file or directory.\n                                                                                 \n  -n name   The extended attribute name.                                         \n  -v value  The extended attribute value. There are three different encoding     \n            methods for the value. If the argument is enclosed in double quotes, \n            then the value is the string inside the quotes. If the argument is   \n            prefixed with 0x or 0X, then it is taken as a hexadecimal number. If \n            the argument begins with 0s or 0S, then it is taken as a base64      \n            encoding.                                                            \n  -x name   Remove the extended attribute.                                       \n  <path>    The file or directory.                                               \n\n-setrep [-R] [-w] <rep> <path> ... :\n  Set the replication level of a file. If <path> is a directory then the command\n  recursively changes the replication factor of all files under the directory tree\n  rooted at <path>.\n                                                                                 \n  -w  It requests that the command waits for the replication to complete. This   \n      can potentially take a very long time.                                     \n  -R  It is accepted for backwards compatibility. It has no effect.              \n\n-stat [format] <path> ... :\n  Print statistics about the file/directory at <path> in the specified format.\n  Format accepts filesize in blocks (%b), group name of owner(%g), filename (%n),\n  block size (%o), replication (%r), user name of owner(%u), modification date\n  (%y, %Y)\n\n-tail [-f] <file> :\n  Show the last 1KB of the file.\n                                             \n  -f  Shows appended data as the file grows. \n\n-test -[defsz] <path> :\n  Answer various questions about <path>, with result via exit status.\n    -d  return 0 if <path> is a directory.\n    -e  return 0 if <path> exists.\n    -f  return 0 if <path> is a file.\n    -s  return 0 if file <path> is greater than zero bytes in size.\n    -z  return 0 if file <path> is zero bytes in size, else return 1.\n\n-text [-ignoreCrc] <src> ... :\n  Takes a source file and outputs the file in text format.\n  The allowed formats are zip and TextRecordInputStream and Avro.\n\n-touchz <path> ... :\n  Creates a file of zero length at <path> with current time as the timestamp of\n  that <path>. An error is returned if the file exists with non-zero length\n\n-usage [cmd ...] :\n  Displays the usage for given command or all commands if none is specified.\n\nGeneric options supported are\n-conf <configuration file>     specify an application configuration file\n-D <property=value>            use value for given property\n-fs <local|namenode:port>      specify a namenode\n-jt <local|resourcemanager:port>    specify a ResourceManager\n-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n\n"}]},"apps":[],"jobName":"paragraph_1632954440622_1673121737","id":"20210929-152720_1753381392","dateCreated":"2021-09-29T15:27:20-0700","dateStarted":"2021-09-29T15:27:37-0700","dateFinished":"2021-09-29T15:27:38-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33528"},{"title":"Exercise 1: HDFS basics operations","text":"# List files and directory present in the root \n\nhdfs dfs -ls /","user":"anonymous","dateUpdated":"2021-09-30T05:06:19-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 6 items\ndrwxrwxrwx   - hdfs  supergroup          0 2017-10-23 09:15 /benchmarks\ndrwxr-xr-x   - hbase supergroup          0 2021-09-23 18:07 /hbase\ndrwxr-xr-x   - solr  solr                0 2017-10-23 09:18 /solr\ndrwxrwxrwt   - hdfs  supergroup          0 2020-01-01 13:30 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /user\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /var\n"}]},"apps":[],"jobName":"paragraph_1632446827593_364879371","id":"20210923-182707_684458834","dateCreated":"2021-09-23T18:27:07-0700","dateStarted":"2021-09-30T04:45:54-0700","dateFinished":"2021-09-30T04:45:57-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33529"},{"text":"# Create a new directory on HDFS\n\nhdfs dfs -mkdir /lab1\n#hdfs dfs -mkdir -p /lab1/blabla","user":"anonymous","dateUpdated":"2021-09-30T04:47:07-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1632446845928_1941571533","id":"20210923-182725_1952851504","dateCreated":"2021-09-23T18:27:25-0700","dateStarted":"2021-09-30T04:47:07-0700","dateFinished":"2021-09-30T04:47:10-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33530"},{"text":"hdfs dfs -ls -h /","user":"anonymous","dateUpdated":"2021-09-30T04:49:31-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 7 items\ndrwxrwxrwx   - hdfs  supergroup          0 2017-10-23 09:15 /benchmarks\ndrwxr-xr-x   - hbase supergroup          0 2021-09-23 18:07 /hbase\ndrwxr-xr-x   - root  supergroup          0 2021-09-30 04:47 /lab1\ndrwxr-xr-x   - solr  solr                0 2017-10-23 09:18 /solr\ndrwxrwxrwt   - hdfs  supergroup          0 2020-01-01 13:30 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /user\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /var\n"}]},"apps":[],"jobName":"paragraph_1632446904142_-718987605","id":"20210923-182824_1729135115","dateCreated":"2021-09-23T18:28:24-0700","dateStarted":"2021-09-30T04:49:07-0700","dateFinished":"2021-09-30T04:49:10-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33531"},{"text":"# Put 5000-8.txt file into HDFS under /lab1 \n\nhdfs dfs -put /home/cloudera/Downloads/5000-8.txt /lab1","user":"anonymous","dateUpdated":"2021-09-30T04:50:11-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1632446949421_908237821","id":"20210923-182909_1863412808","dateCreated":"2021-09-23T18:29:09-0700","dateStarted":"2021-09-30T04:50:11-0700","dateFinished":"2021-09-30T04:50:14-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33532"},{"text":"# View the content of /lab1 directory and determine the size of the file on HDFS\n\nhdfs dfs -ls -h /lab1/","user":"anonymous","dateUpdated":"2021-09-30T04:51:39-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 1 items\n-rw-r--r--   1 root supergroup      1.4 M 2021-09-30 04:50 /lab1/5000-8.txt\n"}]},"apps":[],"jobName":"paragraph_1632448914387_-1235186563","id":"20210923-190154_1854325920","dateCreated":"2021-09-23T19:01:54-0700","dateStarted":"2021-09-30T04:51:39-0700","dateFinished":"2021-09-30T04:51:42-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33533"},{"text":"# Determine the size of the file on HDFS\n\nhdfs dfs -du -h -s /lab1/5000-8.txt","user":"anonymous","dateUpdated":"2021-09-30T04:52:35-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1.4 M  1.4 M  /lab1/5000-8.txt\n"}]},"apps":[],"jobName":"paragraph_1632448951279_-660156874","id":"20210923-190231_1410350585","dateCreated":"2021-09-23T19:02:31-0700","dateStarted":"2021-09-30T04:52:35-0700","dateFinished":"2021-09-30T04:52:38-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33534"},{"text":"# Print the first 5 lines to screen from the file on HDFS\n\nhdfs dfs -cat /lab1/5000-8.txt | head -n 5","user":"anonymous","dateUpdated":"2021-09-30T04:53:03-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"The Project Gutenberg EBook of The Notebooks of Leonardo Da Vinci, Complete\r\nby Leonardo Da Vinci\r\n(#3 in our series by Leonardo Da Vinci)\r\n\r\nCopyright laws are changing all over the world. Be sure to check the\r\ncat: Unable to write to output stream.\n"}]},"apps":[],"jobName":"paragraph_1632449039091_-2015221998","id":"20210923-190359_2097634922","dateCreated":"2021-09-23T19:03:59-0700","dateStarted":"2021-09-30T04:53:03-0700","dateFinished":"2021-09-30T04:53:07-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33535"},{"text":"# Copy lab1 to /lab-hdfscopy on HDFS\n\nhdfs dfs -cp /lab1/5000-8.txt /lab1/5000-8-hdfscopy.txt","user":"anonymous","dateUpdated":"2021-09-30T05:00:40-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1632449669713_1489607476","id":"20210923-191429_790993553","dateCreated":"2021-09-23T19:14:29-0700","dateStarted":"2021-09-30T05:00:40-0700","dateFinished":"2021-09-30T05:00:44-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33536"},{"text":"hdfs dfs -ls /lab1","user":"anonymous","dateUpdated":"2021-09-30T05:01:24-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 2 items\n-rw-r--r--   1 root supergroup    1428841 2021-09-30 05:00 /lab1/5000-8-hdfscopy.txt\n-rw-r--r--   1 root supergroup    1428841 2021-09-30 04:50 /lab1/5000-8.txt\n"}]},"apps":[],"jobName":"paragraph_1632521491574_-257742298","id":"20210924-151131_1910197421","dateCreated":"2021-09-24T15:11:31-0700","dateStarted":"2021-09-30T05:01:24-0700","dateFinished":"2021-09-30T05:01:27-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33537"},{"text":"# Copy lab1/ back to local filesystem and name it lab1_localcopy\n\nhdfs dfs -get /lab1/5000-8.txt home/cloudera/Downloads/5000-8-localcopy.txt","user":"anonymous","dateUpdated":"2021-09-30T11:11:02-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1632522423111_-1011980406","id":"20210924-152703_985345996","dateCreated":"2021-09-24T15:27:03-0700","dateStarted":"2021-09-30T05:03:45-0700","dateFinished":"2021-09-30T05:03:48-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33538"},{"text":"ls /home/cloudera/Downloads","user":"anonymous","dateUpdated":"2021-09-30T05:04:10-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"1800.csv\n2019.csv.gz\n5000-8-localcopy.txt\n5000-8.txt\nAFINN.txt\narbres-publics.csv\nBaby_Names__Beginning_2007.csv\nBaby_Names__Beginning_2007.xml\nbanana-release.zip\nbigunits.jar\nBigUnits.java\nBixiMontrealRentals2018.zip\nBixiMontrealRentals2019.zip\nbooks.xml\nCustomer.csv\ndictionary.tsv\nEmployees.csv\nemployees_db-full-1.0.6.tar.bz2\nFlumeData.zip\nflume-sources-1.0-SNAPSHOT.jar\nfriends.csv\nhadoop-core-1.2.1.jar\nmapper.py\nMedalsByCountry.java\nml-20m.zip\nmorphline.conf\nolympix_data.csv\nolympixmedals.jar\nPokemon.csv\nProduct.csv\nprotocols.txt\nreal_state.csv\nreducer.py\nSales.csv\nsample-data.txt\nschema.xml\nStations_2019_woheader.csv\ntime_zone_map.tsv\nTwitter Dashboard.json\ntwitter.zip\nu.data\nzips.json\n"}]},"apps":[],"jobName":"paragraph_1633003433641_1838897657","id":"20210930-050353_878818758","dateCreated":"2021-09-30T05:03:53-0700","dateStarted":"2021-09-30T05:04:10-0700","dateFinished":"2021-09-30T05:04:10-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33539"},{"text":"hdfs dfs -rm /lab1/5000-8.txt","user":"anonymous","dateUpdated":"2021-09-30T05:04:55-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /lab1/5000-8.txt\n"}]},"apps":[],"jobName":"paragraph_1632526315105_-921266414","id":"20210924-163155_1113890208","dateCreated":"2021-09-24T16:31:55-0700","dateStarted":"2021-09-30T05:04:55-0700","dateFinished":"2021-09-30T05:04:58-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33540"},{"text":"hdfs dfs -ls /lab1","user":"anonymous","dateUpdated":"2021-09-30T05:05:00-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 1 items\n-rw-r--r--   1 root supergroup    1428841 2021-09-30 05:00 /lab1/5000-8-hdfscopy.txt\n"}]},"apps":[],"jobName":"paragraph_1632527351193_-1457299184","id":"20210924-164911_712221995","dateCreated":"2021-09-24T16:49:11-0700","dateStarted":"2021-09-30T05:05:00-0700","dateFinished":"2021-09-30T05:05:03-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33541"},{"text":"hdfs dfs -rm -R /lab1","user":"anonymous","dateUpdated":"2021-09-30T05:05:10-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Deleted /lab1\n"}]},"apps":[],"jobName":"paragraph_1632529116813_1387855528","id":"20210924-171836_137892393","dateCreated":"2021-09-24T17:18:36-0700","dateStarted":"2021-09-30T05:05:10-0700","dateFinished":"2021-09-30T05:05:13-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33542"},{"text":"hdfs dfs -ls /","user":"anonymous","dateUpdated":"2021-09-30T05:05:15-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Found 6 items\ndrwxrwxrwx   - hdfs  supergroup          0 2017-10-23 09:15 /benchmarks\ndrwxr-xr-x   - hbase supergroup          0 2021-09-23 18:07 /hbase\ndrwxr-xr-x   - solr  solr                0 2017-10-23 09:18 /solr\ndrwxrwxrwt   - hdfs  supergroup          0 2020-01-01 13:30 /tmp\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /user\ndrwxr-xr-x   - hdfs  supergroup          0 2017-10-23 09:17 /var\n"}]},"apps":[],"jobName":"paragraph_1632529157876_957497653","id":"20210924-171917_1461436527","dateCreated":"2021-09-24T17:19:17-0700","dateStarted":"2021-09-30T05:05:15-0700","dateFinished":"2021-09-30T05:05:18-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33543"},{"title":"Exercise 2: HDFS basic administrator operations","text":"hdfs dfsadmin -help","user":"anonymous","dateUpdated":"2021-09-30T05:08:54-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh","title":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"hdfs dfsadmin performs DFS administrative commands.\nNote: Administrative commands can only be run with superuser permission.\nThe full syntax is: \n\nhdfs dfsadmin\n\t[-report [-live] [-dead] [-decommissioning]]\n\t[-safemode <enter | leave | get | wait>]\n\t[-saveNamespace]\n\t[-rollEdits]\n\t[-restoreFailedStorage true|false|check]\n\t[-refreshNodes]\n\t[-setQuota <quota> <dirname>...<dirname>]\n\t[-clrQuota <dirname>...<dirname>]\n\t[-setSpaceQuota <quota> <dirname>...<dirname>]\n\t[-clrSpaceQuota <dirname>...<dirname>]\n\t[-finalizeUpgrade]\n\t[-rollingUpgrade [<query|prepare|finalize>]]\n\t[-refreshServiceAcl]\n\t[-refreshUserToGroupsMappings]\n\t[-refreshSuperUserGroupsConfiguration]\n\t[-refreshCallQueue]\n\t[-refresh <host:ipc_port> <key> [arg1..argn]\n\t[-reconfig <datanode|...> <host:ipc_port> <start|status|properties>]\n\t[-printTopology]\n\t[-refreshNamenodes datanode_host:ipc_port]\n\t[-deleteBlockPool datanode_host:ipc_port blockpoolId [force]]\n\t[-setBalancerBandwidth <bandwidth in bytes per second>]\n\t[-fetchImage <local directory>]\n\t[-allowSnapshot <snapshotDir>]\n\t[-disallowSnapshot <snapshotDir>]\n\t[-shutdownDatanode <datanode_host:ipc_port> [upgrade]]\n\t[-getDatanodeInfo <datanode_host:ipc_port>]\n\t[-metasave filename]\n\t[-triggerBlockReport [-incremental] <datanode_host:ipc_port>]\n\t[-listOpenFiles]\n\t[-help [cmd]]\n\n-report [-live] [-dead] [-decommissioning]:\n\tReports basic filesystem information and statistics. \n\tThe dfs usage can be different from \"du\" usage, because it\n\tmeasures raw space used by replication, checksums, snapshots\n\tand etc. on all the DNs.\n\tOptional flags may be used to filter the list of displayed DNs.\n\n-safemode <enter|leave|get|wait>:  Safe mode maintenance command.\n\t\tSafe mode is a Namenode state in which it\n\t\t\t1.  does not accept changes to the name space (read-only)\n\t\t\t2.  does not replicate or delete blocks.\n\t\tSafe mode is entered automatically at Namenode startup, and\n\t\tleaves safe mode automatically when the configured minimum\n\t\tpercentage of blocks satisfies the minimum replication\n\t\tcondition.  Safe mode can also be entered manually, but then\n\t\tit can only be turned off manually as well.\n\n-saveNamespace:\tSave current namespace into storage directories and reset edits log.\n\t\tRequires safe mode.\n\n-rollEdits:\tRolls the edit log.\n\n-restoreFailedStorage:\tSet/Unset/Check flag to attempt restore of failed storage replicas if they become available.\n\n-refreshNodes: \tUpdates the namenode with the set of datanodes allowed to connect to the namenode.\n\n\t\tNamenode re-reads datanode hostnames from the file defined by \n\t\tdfs.hosts, dfs.hosts.exclude configuration parameters.\n\t\tHosts defined in dfs.hosts are the datanodes that are part of \n\t\tthe cluster. If there are entries in dfs.hosts, only the hosts \n\t\tin it are allowed to register with the namenode.\n\n\t\tEntries in dfs.hosts.exclude are datanodes that need to be \n\t\tdecommissioned. Datanodes complete decommissioning when \n\t\tall the replicas from them are replicated to other datanodes.\n\t\tDecommissioned nodes are not automatically shutdown and \n\t\tare not chosen for writing new replicas.\n\n-finalizeUpgrade: Finalize upgrade of HDFS.\n\t\tDatanodes delete their previous version working directories,\n\t\tfollowed by Namenode doing the same.\n\t\tThis completes the upgrade process.\n\n-rollingUpgrade [<query|prepare|finalize>]:\n     query: query the current rolling upgrade status.\n   prepare: prepare a new rolling upgrade.\n  finalize: finalize the current rolling upgrade.\n-metasave <filename>: \tSave Namenode's primary data structures\n\t\tto <filename> in the directory specified by hadoop.log.dir property.\n\t\t<filename> is overwritten if it exists.\n\t\t<filename> will contain one line for each of the following\n\t\t\t1. Datanodes heart beating with Namenode\n\t\t\t2. Blocks waiting to be replicated\n\t\t\t3. Blocks currrently being replicated\n\t\t\t4. Blocks waiting to be deleted\n\n-setQuota <quota> <dirname>...<dirname>: Set the quota <quota> for each directory <dirName>.\n\t\tThe directory quota is a long integer that puts a hard limit\n\t\ton the number of names in the directory tree\n\t\tFor each directory, attempt to set the quota. An error will be reported if\n\t\t1. N is not a positive integer, or\n\t\t2. User is not an administrator, or\n\t\t3. The directory does not exist or is a file.\n\t\tNote: A quota of 1 would force the directory to remain empty.\n\n-clrQuota <dirname>...<dirname>: Clear the quota for each directory <dirName>.\n\t\tFor each directory, attempt to clear the quota. An error will be reported if\n\t\t1. the directory does not exist or is a file, or\n\t\t2. user is not an administrator.\n\t\tIt does not fault if the directory has no quota.\n-setSpaceQuota <quota> <dirname>...<dirname>: Set the disk space quota <quota> for each directory <dirName>.\n\t\tThe space quota is a long integer that puts a hard limit\n\t\ton the total size of all the files under the directory tree.\n\t\tThe extra space required for replication is also counted. E.g.\n\t\ta 1GB file with replication of 3 consumes 3GB of the quota.\n\n\t\tQuota can also be specified with a binary prefix for terabytes,\n\t\tpetabytes etc (e.g. 50t is 50TB, 5m is 5MB, 3p is 3PB).\n\t\tFor each directory, attempt to set the quota. An error will be reported if\n\t\t1. N is not a positive integer, or\n\t\t2. user is not an administrator, or\n\t\t3. the directory does not exist or is a file, or\n\n-clrSpaceQuota <dirname>...<dirname>: Clear the disk space quota for each directory <dirName>.\n\t\tFor each directory, attempt to clear the quota. An error will be reported if\n\t\t1. the directory does not exist or is a file, or\n\t\t2. user is not an administrator.\n\t\tIt does not fault if the directory has no quota.\n-refreshServiceAcl: Reload the service-level authorization policy file\n\t\tNamenode will reload the authorization policy file.\n\n-refreshUserToGroupsMappings: Refresh user-to-groups mappings\n\n-refreshSuperUserGroupsConfiguration: Refresh superuser proxy groups mappings\n\n-refreshCallQueue: Reload the call queue from config\n\n-refresh: Arguments are <hostname:port> <resource_identifier> [arg1..argn]\n\tTriggers a runtime-refresh of the resource specified by <resource_identifier>\n\ton <hostname:port>. All other args after are sent to the host.\n\n-reconfig <datanode|...> <host:ipc_port> <start|status|properties>:\n\tStarts or gets the status of a reconfiguration operation, \n\tor gets a list of reconfigurable properties.\n\tThe second parameter specifies the node type.\n\tCurrently, only reloading DataNode's configuration is supported.\n\n-printTopology: Print a tree of the racks and their\n\t\tnodes as reported by the Namenode\n\n-refreshNamenodes: Takes a datanodehost:port as argument,\n\t\tFor the given datanode, reloads the configuration files,\n\t\tstops serving the removed block-pools\n\t\tand starts serving new block-pools\n\n-deleteBlockPool: Arguments are datanodehost:port, blockpool id\n\t\t and an optional argument \"force\". If force is passed,\n\t\t block pool directory for the given blockpool id on the given\n\t\t datanode is deleted along with its contents, otherwise\n\t\t the directory is deleted only if it is empty. The command\n\t\t will fail if datanode is still serving the block pool.\n\t\t   Refer to refreshNamenodes to shutdown a block pool\n\t\t service on a datanode.\n\n-setBalancerBandwidth <bandwidth>:\n\tChanges the network bandwidth used by each datanode during\n\tHDFS block balancing.\n\n\t\t<bandwidth> is the maximum number of bytes per second\n\t\tthat will be used by each datanode. This value overrides\n\t\tthe dfs.balance.bandwidthPerSec parameter.\n\n\t\t--- NOTE: The new value is not persistent on the DataNode.---\n\n-fetchImage <local directory>:\n\tDownloads the most recent fsimage from the Name Node and saves it in\tthe specified local directory.\n\n-allowSnapshot <snapshotDir>:\n\tAllow snapshots to be taken on a directory.\n\n-disallowSnapshot <snapshotDir>:\n\tDo not allow snapshots to be taken on a directory any more.\n\n-shutdownDatanode <datanode_host:ipc_port> [upgrade]\n\tSubmit a shutdown request for the given datanode. If an optional\n\t\"upgrade\" argument is specified, clients accessing the datanode\n\twill be advised to wait for it to restart and the fast start-up\n\tmode will be enabled. When the restart does not happen in time,\n\tclients will timeout and ignore the datanode. In such case, the\n\tfast start-up mode will also be disabled.\n\n-getDatanodeInfo <datanode_host:ipc_port>\n\tGet the information about the given datanode. This command can\n\tbe used for checking if a datanode is alive.\n\n-triggerBlockReport [-incremental] <datanode_host:ipc_port>\n\tTrigger a block report for the datanode.\n\tIf 'incremental' is specified, it will be an incremental\n\tblock report; otherwise, it will be a full block report.\n\n-listOpenFiles\n\tList all open files currently managed by the NameNode along\n\twith client name and client machine accessing them.\n\n-help [cmd]: \tDisplays help for the given command or all commands if none\n\t\tis specified.\n\n\nGeneric options supported are\n-conf <configuration file>     specify an application configuration file\n-D <property=value>            use value for given property\n-fs <local|namenode:port>      specify a namenode\n-jt <local|resourcemanager:port>    specify a ResourceManager\n-files <comma separated list of files>    specify comma separated files to be copied to the map reduce cluster\n-libjars <comma separated list of jars>    specify comma separated jar files to include in the classpath.\n-archives <comma separated list of archives>    specify comma separated archives to be unarchived on the compute machines.\n\nThe general command line syntax is\nbin/hadoop command [genericOptions] [commandOptions]\n\n"}]},"apps":[],"jobName":"paragraph_1632956738503_682582215","id":"20210929-160538_418734984","dateCreated":"2021-09-29T16:05:38-0700","dateStarted":"2021-09-29T16:05:51-0700","dateFinished":"2021-09-29T16:05:52-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33544"},{"text":"# Print the HDFS report\n# Get the size on HDFS\n\nhdfs dfsadmin -report","user":"anonymous","dateUpdated":"2021-09-30T11:21:44-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Configured Capacity: 58531520512 (54.51 GB)\nPresent Capacity: 39541059584 (36.83 GB)\nDFS Remaining: 38666682368 (36.01 GB)\nDFS Used: 874377216 (833.87 MB)\nDFS Used%: 2.21%\nUnder replicated blocks: 0\nBlocks with corrupt replicas: 0\nMissing blocks: 0\nMissing blocks (with replication factor 1): 938\n\n-------------------------------------------------\nLive datanodes (1):\n\nName: 10.0.2.15:50010 (quickstart.cloudera)\nHostname: quickstart.cloudera\nDecommission Status : Normal\nConfigured Capacity: 58531520512 (54.51 GB)\nDFS Used: 874377216 (833.87 MB)\nNon DFS Used: 16010407936 (14.91 GB)\nDFS Remaining: 38666682368 (36.01 GB)\nDFS Used%: 1.49%\nDFS Remaining%: 66.06%\nConfigured Cache Capacity: 0 (0 B)\nCache Used: 0 (0 B)\nCache Remaining: 0 (0 B)\nCache Used%: 100.00%\nCache Remaining%: 0.00%\nXceivers: 2\nLast contact: Fri Sep 24 18:18:25 PDT 2021\n\n\n"}]},"apps":[],"jobName":"paragraph_1632531326910_369644516","id":"20210924-175526_1196917765","dateCreated":"2021-09-24T17:55:26-0700","dateStarted":"2021-09-24T18:18:24-0700","dateFinished":"2021-09-24T18:18:27-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33545"},{"text":"# Check the entire HDFS filesystem for problem\n\nhdfs fsck /","user":"anonymous","dateUpdated":"2021-09-30T05:20:50-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Connecting to namenode via http://quickstart.cloudera:50070/fsck?ugi=root&path=%2F\nFSCK started by root (auth:SIMPLE) from /10.0.2.15 for path / at Thu Sep 30 05:20:53 PDT 2021\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n....................................................................................................\n............................................................Status: HEALTHY\n Total size:\t863172472 B\n Total dirs:\t178\n Total files:\t960\n Total symlinks:\t\t0\n Total blocks (validated):\t957 (avg. block size 901956 B)\n Minimally replicated blocks:\t957 (100.0 %)\n Over-replicated blocks:\t0 (0.0 %)\n Under-replicated blocks:\t0 (0.0 %)\n Mis-replicated blocks:\t\t0 (0.0 %)\n Default replication factor:\t1\n Average block replication:\t1.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0 (0.0 %)\n Number of data-nodes:\t\t1\n Number of racks:\t\t1\nFSCK ended at Thu Sep 30 05:20:53 PDT 2021 in 193 milliseconds\n\n\nThe filesystem under path '/' is HEALTHY\n"}]},"apps":[],"jobName":"paragraph_1632529866873_-661363400","id":"20210924-173106_471702988","dateCreated":"2021-09-24T17:31:06-0700","dateStarted":"2021-09-30T05:20:50-0700","dateFinished":"2021-09-30T05:20:53-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33546"},{"text":"# View and verify the consistency of HDFS files blocks\n\nhdfs fsck /user/cloudera -files -blocks","user":"anonymous","dateUpdated":"2021-09-30T11:26:45-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Connecting to namenode via http://quickstart.cloudera:50070/fsck?ugi=root&files=1&blocks=1&path=%2Fuser%2Fcloudera\nFSCK started by root (auth:SIMPLE) from /10.0.2.15 for path /user/cloudera at Thu Sep 30 11:26:48 PDT 2021\n/user/cloudera <dir>\nStatus: HEALTHY\n Total size:\t0 B\n Total dirs:\t1\n Total files:\t0\n Total symlinks:\t\t0\n Total blocks (validated):\t0\n Minimally replicated blocks:\t0\n Over-replicated blocks:\t0\n Under-replicated blocks:\t0\n Mis-replicated blocks:\t\t0\n Default replication factor:\t1\n Average block replication:\t0.0\n Corrupt blocks:\t\t0\n Missing replicas:\t\t0\n Number of data-nodes:\t\t1\n Number of racks:\t\t1\nFSCK ended at Thu Sep 30 11:26:48 PDT 2021 in 0 milliseconds\n\n\nThe filesystem under path '/user/cloudera' is HEALTHY\n"}]},"apps":[],"jobName":"paragraph_1632531096483_-1174549342","id":"20210924-175136_678238123","dateCreated":"2021-09-24T17:51:36-0700","dateStarted":"2021-09-30T11:26:45-0700","dateFinished":"2021-09-30T11:26:48-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33547"},{"text":"# Print the HDFS safe mode status\n\nhdfs dfsadmin -safemode get","user":"anonymous","dateUpdated":"2021-09-30T11:27:16-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Safe mode is OFF\n"}]},"apps":[],"jobName":"paragraph_1632532704913_2113640971","id":"20210924-181824_270094213","dateCreated":"2021-09-24T18:18:24-0700","dateStarted":"2021-09-30T11:27:16-0700","dateFinished":"2021-09-30T11:27:19-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33548"},{"text":"# Get the HDFS block size\n\nhdfs getconf -confKey dfs.blocksize ","user":"anonymous","dateUpdated":"2021-09-30T05:12:53-0700","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"134217728\n"}]},"apps":[],"jobName":"paragraph_1632532825864_166210546","id":"20210924-182025_1997124095","dateCreated":"2021-09-24T18:20:25-0700","dateStarted":"2021-09-24T18:35:58-0700","dateFinished":"2021-09-24T18:35:59-0700","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:33549"},{"user":"anonymous","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"sh","editOnDblClick":false,"completionSupport":false},"editorMode":"ace/mode/sh"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1633021421983_-2070623926","id":"20210930-100341_1976144919","dateCreated":"2021-09-30T10:03:41-0700","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:33550"}],"name":"Course 2: HDFS","id":"2GJS666VQ","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"sh:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}